# 统计学三大检验

F 检验适用于检验模型的线性约束（$\beta_0=\beta_1=\dots=\beta_n=0$）

如果**模型是非线性的**，或**约束是非线性的**，或**扰动项是非正态的**，F 检验将不再适用

通常需要采用 **LR、Wald、LM** 其中之一来检验约束条件是否成立

这三个检验方法是：**渐进等价的**，他们所用统计量的小样本分布是未知的，但都**渐进服从自由度为约束个数的卡方分布**

似然比检验 (Likelihood Ratio Test，LR)、沃尔德检验 (Wald Test，W)、拉格朗日乘数检验 (Lagrange Multiplier，LM) 是三种基于最大似然法的大样本检验方法





## 似然比检验（LR）

**基本思想**：检验增加约束后（模型变得简单），似然函数最大值是否有显著的减少

> 把约束理解为“约束参数为 0”，从而模型变得简单

具体解释：将较**简单模型（有约束模型，$\beta_0=\beta_1=\dots=\beta_n=0$）的似然函数最大值**与较**复杂模型的似然函数最大值**进行比较，因为简单模型的参数少，似然函数自然要小于复杂模型的似然函数，但如果简单模型的似然函数最大值很显著地小于复杂模型的似然函数最大值，则认为复杂模型更合理，判断是否显著的标准就是使用**似然比检验**

似然比检验不仅仅可以检验一个参数，还可以检验两个嵌套模型多个参数整体上是否显著为 0



### 原假设

$$
H_0:g(\beta)=C
$$



### 似然比

$$
\lambda=\frac{L(\tilde\beta,\tilde\sigma^2)}{L(\hat\beta,\hat\sigma^2)}
$$

$L(\tilde\beta,\tilde\sigma^2)$ ：有约束模型（简单模型）的似然函数值

$L(\hat\beta,\hat\sigma^2)$ ：无约束模型（复杂模型）的似然函数值



显然 $0\le\lambda\le1$ ，如果原假设为真，则 $\lambda$ 趋近于 1，如果 $\lambda$ 太小，则约束无效，拒绝原假设



### LR 检验统计量

**对于大样本，LR 统计量为：**
$$
LR=-2\ln\lambda=-2\ln\frac{L(\tilde\beta,\tilde\sigma^2)}{L(\hat\beta,\hat\sigma^2)}=2[\ln L(\hat\beta,\hat\sigma^2)-\ln L(\tilde\beta,\tilde\sigma^2)]\sim \chi^2(q)
$$

> ln 不影响最大值的取值，同时转除法为减法

$q$ ：参数个数差（约束条件个数）



### LR 统计量的另一种表达方式

$$
LR=-2\ln\lambda=n(\ln e_*^Te_*-lne^Te)\sim\chi^2(q)
$$

$e_*$ ：有约束模型（简单模型）的残差平方和

$e$ ：无约束模型（复制模型）的残差平方和

线性回归的最大似然函数为：
$$
L(\mathbf{Y},\mathbf{x};\mathbf{\beta},\sigma^2)=(2\pi\sigma^2)^{-\frac{n}{2}}exp(-\frac{(\mathbf{Y}-\mathbf{X} \boldsymbol{\beta})^{T}(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}) }{2\sigma^2})
$$
对数似然函数为：
$$
\begin{array}{c}\ln L_{n}\left(\boldsymbol{\beta}, \sigma^{2}\right)=-\frac{n}{2} \ln 2 \pi-\frac{n}{2} \ln \sigma^{2}-\frac{1}{2 \sigma^{2}}(\mathbf{y}-\mathbf{X} \boldsymbol{\beta})^{T}(\mathbf{y}-\mathbf{X}\boldsymbol{\beta}) \\ \end{array}
$$


另一阶导为 0，最大似然估计法得到的参数为：
$$
\hat{\boldsymbol{\beta}}_{ML}=\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \mathbf{y} \\ \hat{\sigma}_{ML}^{2}=\frac{1}{n} \mathbf{e}^{T} \mathbf{e}
$$
将估计参数代入对数似然函数，可得最大对数似然估计值为：
$$
l=\ln l=\frac{n}{2}[\ln \frac{n}{2\pi}-1-\ln (e^Te)]
$$
LR 统计量的另一种表达方式：
$$
\begin{aligned}
LR&=-2\ln\lambda\\
&=-2\ln\frac{L(\tilde\beta,\tilde\sigma^2)}{L(\hat\beta,\hat\sigma^2)}\\
&=-2[\ln L(\tilde\beta,\tilde\sigma^2)-\ln L(\hat\beta,\hat\sigma^2)]\\
&=-2\{ \frac{n}{2}[\ln \frac{n}{2\pi}-1-\ln (e_*^Te_*)]-\frac{n}{2}[\ln \frac{n}{2\pi}-1-\ln (e^Te)]\}\\ 
&=n[\ln (e_*^Te_*)-\ln (e^Te)]\\
&\sim \chi^2(q)
\end{aligned}
$$
$e_*^Te_*$ ：有约束模型（简单模型）的残差平方和

$e^Te$ ：无约束模型（复制模型）的残差平方和



### 拒绝域

$$
\{LR\ge\chi^2_{1-\alpha}(q)\}
$$





## Wald 检验

**基本思想**：如果约束是有效的，那么在无约束的情况下，估计出来的估计量应该渐进地满足约束条件，因为 MLE 是一致的



### 原假设

$$
H_0:g(\beta)=C
$$

如果 $g(\beta^{MLE})-C$ 显著异于 0，即约束条件无效，拒绝原假设，认为无约束的模型（复杂模型）更合理



### Wald 检验统计量

$$
W=(g(\hat{\beta})-C)^T[Var(g(\hat{\beta})-C)]^{-1}(g(\hat{\beta})-C)\sim\chi^2(q)
$$





### 线性约束条件下的 Wald 检验

原假设：$H_0:R\beta=r$
$$
W=(R\hat{\beta}-r)^T[R\hat{\sigma}^2(X^TX)^{-1}R^T]^{-1}(R\hat{\beta}-r)\sim\chi^2(q)
$$


### 拒绝域

$$
\{W\ge\chi^2_{\alpha}(q)\}
$$



### Wald 统计量的另一种表达方式

$$
W=\frac{n(e_*^Te_*-e^Te)}{e^Te}\sim\chi^2(q)
$$

$e_*^Te_*$ ：有约束模型（简单模型）的残差平方和

$e^Te$ ：无约束模型（复制模型）的残差平方和





## 拉格朗日乘子检验（LM）

拉格朗日乘子检验（LM），又称 **Score 检验**，该检验基于约束模型无需估计无约束模型

**基本思想**：在约束条件下，可以用拉格朗日方法构造目标函数，如果约束有效，则最大化拉格朗日函数所得估计量应位于最大化无约束所得参数估计值附近



### 原假设

约束条件：
$$
H_0:g(\theta)=C
$$
在此约束的条件下，最大化对数似然函数：
$$
\ln L^*(\theta)=\ln L(\theta)+\lambda^T[g(\theta)-C]
$$
有约束条件下的最大化问题就是求解：
$$
\frac{\partial \ln L^*(\theta)}{\partial\theta}=\frac{\partial \ln L(\theta)}{\partial\theta}+\left[\frac{\partial g(\theta)}{\partial \theta^T}\right]^T\lambda=0\\
\frac{\partial \ln L^*(\theta)}{\partial\lambda}=g(\theta)-C=0
$$
如果约束成立， 对数似然函数值不会有显著变化

这就意味着在一阶条件下，$\lambda$ **应该趋于 0**，因此，约束条件是否成立检验转化成检验：
$$
H_0:\lambda=0
$$
直接检验 $\lambda=0$ 困难，转为等价的检验方法，即如果约束条件成立，在**约束估计值处计算对数似然函数的导数应该近似为 0**， 如果该值显著异于 0， 则约束条件不成立， 拒绝原假设

**对数似然函数的导数就是得分向量**， 因此 LM 检验就是检验约束条件下参数估计值的得分向量值是否显著异于 0， 因而 LM 检验又称为 **Score 检验**



在最大似然估计过程中， 通过解似然方程 $S(\hat{\theta})=0$ 可以求出无约束估计量 $\hat{\theta}$ 

计算有约束估计量 $\tilde{\theta}$ 在此处的得分，如果 $S(\tilde{\theta})$ 趋于 0，则约束有效



### LM 统计量

在原假设成立的条件下：
$$
LM=S(\tilde{\theta})^TI(\tilde{\theta})^{-1}S(\tilde{\theta})\sim\chi^2(q)
$$
将有关量代入上式得：
$$
LM=\frac{ne_*^TX(X^TX)^{-1}X^Te_*}{e_*^Te_*}=nR^2\sim\chi^2(q)
$$
$e_*^Te_*$ ：有约束模型（简单模型）的残差平方和

$R^2$ ：$e_*$ 对 $X$ 回归的拟合优度



### 拒绝域

$$
LM=nR^2\ge\chi_\alpha^2(q)
$$



### LM 统计量的另一种表达方式

$$
LM=\frac{n(e_*^Te_*-e^Te)}{e_*^Te_*}\sim\chi^2(q)
$$

$e_*^Te_*$ ：有约束模型（简单模型）的残差平方和

$e^Te$ ：无约束模型（复制模型）的残差平方和





## Wald 检验与 LM 检验

Wald 统计量实际上是将 LM 统计量 $\beta$ 的标准误换成了 $\hat{\beta}$

LM 统计量的本质是：
$$
\left({\frac {\hat {\beta}-\beta }{sd(\beta)}}\right)^2
$$
Wald 检验统计量的本质是：
$$
\left({\frac {\hat {\beta}-\beta }{sd(\hat{\beta})}}\right)^2
$$
Wald 检验只需估计**无约束模型**，但需要计算渐进协方差矩阵





## LR  Wald  LM

一般情况下：
$$
Wald\ge LR\ge LM
$$
三大检验中，**LR 检验用到的信息最多**，其结果较 Wald 检验更可靠，尤其是当样本量较小时